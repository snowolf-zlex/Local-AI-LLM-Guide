# ğŸ”„ Safetensors æ ¼å¼è½¬æ¢ä¸º GGUF æ ¼å¼å¹¶å¯¼å…¥ Ollama

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•å°† HuggingFace æ ¼å¼çš„ safetensors æ¨¡å‹è½¬æ¢ä¸º GGUF æ ¼å¼ï¼Œå¹¶å¯¼å…¥åˆ° Ollama ä¸­ä½¿ç”¨ã€‚

## ğŸ“‹ å‰ç½®çŸ¥è¯†

- **Safetensors**: HuggingFace æ¨å‡ºçš„å®‰å…¨å¼ é‡æ ¼å¼ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹æƒé‡
- **GGUF**: llama.cpp ä½¿ç”¨çš„æ¨¡å‹æ ¼å¼ï¼Œæ”¯æŒé‡åŒ–å‹ç¼©ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²
- **Ollama**: æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå·¥å…·ï¼Œæ”¯æŒ GGUF æ ¼å¼æ¨¡å‹

---

## ğŸ“¥ 1. å…‹éš† llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

## ğŸ”§ 2. æ„å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
conda create -n llamacpp python=3.10
conda activate llamacpp
```

## ğŸ“¦ 3. å®‰è£… llama.cpp ä¾èµ–ç¯å¢ƒ

C++ å’Œ Python ç¯å¢ƒäºŒé€‰ä¸€å³å¯ï¼Œå»ºè®®ä¸¤ä¸ªéƒ½å®‰è£…ã€‚

### 1) ç¼–è¯‘ C++ ç¯å¢ƒ

é‡åŒ–éœ€è¦ä½¿ç”¨ C++ å·¥å…·ã€‚

```bash
cd llama.cpp
mkdir build
cd build
cmake ..
make -j
```

### 2) å®‰è£… Python ç¯å¢ƒ

æ¨¡å‹è½¬æ¢éœ€è¦ä½¿ç”¨ Python è„šæœ¬ã€‚

```bash
conda activate llamacpp
pip install -r requirements.txt
```

## ğŸ”„ 4. æ¨¡å‹è½¬æ¢

### Python æ–¹å¼

ä» safetensors æ ¼å¼è½¬æ¢ä¸º GGUF æ ¼å¼ï¼ˆQ8_0 é‡åŒ–ï¼‰ï¼š

```bash
cd llama.cpp
python convert_hf_to_gguf.py ../your_model_path \
    --outfile /your_output_path/model.gguf \
    --outtype q8_0
```

è½¬æ¢ safetensors æ¨¡å‹ä¸º Ollama æ ¼å¼ï¼ˆF16ï¼‰ï¼š

```bash
python convert_hf_to_gguf.py ../qwen2-vl-2b --outtype f16
```

### C++ æ–¹å¼

æ³¨æ„ï¼šæ­¤æ–¹å¼éœ€è¦å…ˆç¼–è¯‘ llama.cppï¼Œç„¶ååœ¨ `build/bin` è·¯å¾„ä¸‹æ“ä½œã€‚

<img width="1138" alt="é‡åŒ–å·¥å…·" src="https://github.com/user-attachments/assets/0cfa43af-5f2d-4482-a6c1-594c285a003e" />

Q8_0 é‡åŒ–ï¼š

```bash
llama.cpp/build/bin/llama-quantize ../qwen2-vl-2b/qwen2-vl-2B-F16.gguf q8_0
```

Q4_0 é‡åŒ–ï¼š

```bash
llama.cpp/build/bin/llama-quantize ../qwen2-vl-2b/qwen2-vl-2B-F16.gguf q4_0
```

æ¨¡å‹è½¬æ¢æˆåŠŸï¼š

<img width="1082" alt="è½¬æ¢æˆåŠŸ" src="https://github.com/user-attachments/assets/a3e24aa4-7d2f-4302-a1a7-fac4cf895c33" />

è·å¾—å¤šä¸ª GGUF æ–‡ä»¶ï¼š

<img width="515" alt="GGUFæ–‡ä»¶" src="https://github.com/user-attachments/assets/4d134d3c-9b21-4a85-9764-262c5d183b57" />

---

## ğŸ“¥ 5. æ¨¡å‹å¯¼å…¥ Ollama

### åˆ›å»º Modelfile

```bash
echo '/path/to/your_model.gguf' > Modelfile
```

### åˆ›å»ºï¼ˆå¯¼å…¥ï¼‰æ¨¡å‹

```bash
ollama create model_name -f /path/to/Modelfile
```

æˆåŠŸè¾“å‡ºç¤ºä¾‹ï¼š

```text
(llamacpp) jetson@jetson-orin-nx-super:~/qwen2-vl-2b$ ollama create qwen2-vl-2b -f Modelfile
gathering model components
copying file sha256:1709aa285974b03259b129f2d5bd819beee9a44a0c3d79ca82291fe41838e3d7 100%
parsing GGUF
using existing layer sha256:1709aa285974b03259b129f2d5bd819beee9a44a0c3d79ca82291fe41838e3d7
writing manifest
success
```

### æŸ¥çœ‹å·²å¯¼å…¥çš„æ¨¡å‹

```bash
ollama list
```

è¾“å‡ºç¤ºä¾‹ï¼š

```text
(llamacpp) jetson@jetson-orin-nx-super:~/qwen2-vl-2b$ ollama list
NAME                  ID              SIZE      MODIFIED
qwen2-vl-2b:latest    41a2703169aa    1.8 GB    20 seconds ago
```

## ğŸ¯ è¿è¡Œæ¨¡å‹

```bash
ollama run qwen2-vl-2b
```

---

## ğŸ“š é™„å½•ï¼šé‡åŒ–ç±»å‹è¯´æ˜

| ç±»å‹ | è¯´æ˜ | å¤§å° | ç²¾åº¦ |
|------|------|------|------|
| F16 | å…¨ç²¾åº¦ FP16 | 100% | æœ€é«˜ |
| Q8_0 | 8-bit é‡åŒ– | ~50% | é«˜ |
| Q4_0 | 4-bit é‡åŒ– | ~25% | ä¸­ç­‰ |
| Q4_K_M | 4-bit K-quantization | ~25% | ä¸­ç­‰ |
